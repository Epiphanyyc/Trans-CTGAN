{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc</th>\n",
       "      <th>AUC</th>\n",
       "      <th>F1_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>2.75</td>\n",
       "      <td>0.032865</td>\n",
       "      <td>0.000801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>5.90</td>\n",
       "      <td>0.069439</td>\n",
       "      <td>0.074687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>3.85</td>\n",
       "      <td>0.045534</td>\n",
       "      <td>0.075389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.035053</td>\n",
       "      <td>-0.022103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>4.00</td>\n",
       "      <td>0.044560</td>\n",
       "      <td>-0.075337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Acc       AUC  F1_Score\n",
       "lr   2.75  0.032865  0.000801\n",
       "dt   5.90  0.069439  0.074687\n",
       "rf   3.85  0.045534  0.075389\n",
       "mlp  3.00  0.035053 -0.022103\n",
       "svm  4.00  0.044560 -0.075337"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# third party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "package_path = '../CTAB-GAN-main'\n",
    "\n",
    "sys.path.append(package_path)\n",
    "\n",
    "from model.ctabgan import CTABGAN\n",
    "# Importing the evaluation metrics \n",
    "from model.eval.evaluation import get_utility_metrics,stat_sim,privacy_metrics\n",
    "# Importing standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "# Specifying the list of classifiers to conduct ML utility evaluation\n",
    "real_path = \"../CTAB-GAN-main/Real_Datasets/Adult3.csv\"\n",
    "#real_path = '../CTGAN-main/CTGAN-main/examples/csv/train_clean.csv'\n",
    "#real_path = \"../synthcity-main/tutorials/covertype_preprocessed.csv\"\n",
    "#fake_paths = [\"../synthcity-main/tutorials/DDPM-cover-5.csv\"]\n",
    "\n",
    "#real_path = r\"C:\\Users\\26332\\Desktop\\Churn_Modelling_processed.csv\"\n",
    "#fake_paths = [r\"..\\synthcity-main\\tutorials\\duanwen-trans-adu.csv\"]\n",
    "# real_path = \"G:/DataSets/Adult.csv\"\n",
    "# real_path = 'G:/DataSets/adult_processed_0.csv'\n",
    "fake_paths = [\"G:/DataSets/Fake_Dataset/TransCTGAN-finaladu200_5.csv\"]\n",
    "# fake_paths = [\"G:/DataSets/Fake_Dataset/TransCTGAN-200eAdult3.csv\"]\n",
    "# real_path = \"../CTAB-GAN-main/Real_Datasets/Adult3.csv\"\n",
    "#fake_paths = [\"../fakeData_CTGAN-Adult.csv\"]\n",
    "classifiers_list = [\"lr\",\"dt\",\"rf\",\"mlp\",\"svm\"]\n",
    "# Storing and presenting the results as a dataframe\n",
    "result_mat = get_utility_metrics(real_path,fake_paths,\"MinMax\",classifiers_list, test_ratio = 0.20)\n",
    "result_df  = pd.DataFrame(result_mat,columns=[\"Acc\",\"AUC\",\"F1_Score\"])\n",
    "result_df.index = classifiers_list\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence: -0.27343873333828594\n",
      "Jensen-Shannon Divergence: 0.13372765812366655\n",
      "Wasserstein Distance: 10.007742818711975\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "\n",
    "    epsilon = 1e-10\n",
    "    p = np.clip(p, epsilon, 1)\n",
    "    q = np.clip(q, epsilon, 1)\n",
    "    return np.sum(p * np.log(p / q))\n",
    "\n",
    "def js_divergence(p, q):\n",
    "\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * (kl_divergence(p, m) + kl_divergence(q, m))\n",
    "\n",
    "def wasserstein_distance(p, q):\n",
    "\n",
    "    return np.sum(np.abs(np.cumsum(p) - np.cumsum(q)))\n",
    "\n",
    "def handle_missing_data(data):\n",
    "\n",
    "    return data.dropna()\n",
    "\n",
    "def handle_zero_variance_columns(data):\n",
    "\n",
    "    return data.loc[:, data.var() > 0]\n",
    "\n",
    "def calculate_metrics(real_data, generated_data):\n",
    "\n",
    "    real_data = handle_missing_data(real_data)\n",
    "    generated_data = handle_missing_data(generated_data)\n",
    "\n",
    "    real_data = handle_zero_variance_columns(real_data)\n",
    "    generated_data = handle_zero_variance_columns(generated_data)\n",
    "\n",
    "    real_data = (real_data - real_data.mean(axis=0)) / real_data.std(axis=0)\n",
    "    generated_data = (generated_data - generated_data.mean(axis=0)) / generated_data.std(axis=0)\n",
    "\n",
    "    if real_data.isnull().any().any() or generated_data.isnull().any().any():\n",
    "        print(\"Warning: There are still NaN values after handling!\")\n",
    "        return None, None, None\n",
    "    \n",
    "    real_hist = np.histogram(real_data, bins=30, density=True)[0]\n",
    "    generated_hist = np.histogram(generated_data, bins=30, density=True)[0]\n",
    "\n",
    "    kl = kl_divergence(real_hist, generated_hist)\n",
    "    \n",
    "    jsd = js_divergence(real_hist, generated_hist)\n",
    "\n",
    "    wasserstein = wasserstein_distance(real_hist, generated_hist)\n",
    "\n",
    "    return kl, jsd, wasserstein\n",
    "\n",
    "def load_data(path):\n",
    "    return pd.read_csv(path)\n",
    "#real_path = '../CTGAN-main/CTGAN-main/examples/csv/train_clean.csv'\n",
    "#real_path = \"../CTAB-GAN-main/Real_Datasets/Adult3.csv\"\n",
    "#real_path = \"../CTAB-GAN-main/Real_Datasets/CreditLong2.csv\"\n",
    "#real_path = \"../synthcity-main/tutorials/covertype_preprocessed.csv\"\n",
    "#fake_path = \"../synthcity-main/tutorials/ETransCTGAN-Cov_5.csv\"\n",
    "#fake_path = \"../synthcity-main/tutorials/newtrans_calc_pro_cov.csv\"\n",
    "\n",
    "#fake_path = r\"..\\synthcity-main\\tutorials\\duanwen-trans-cre.csv\"\n",
    "# fake_path = \"G:/DataSets/Fake_Dataset/CTABGAN+-Churn300e2.csv\"\n",
    "#real_path = \"G:/DataSets/Churn.csv\"\n",
    "# real_path = \"G:/DataSets/Adult.csv\"\n",
    "# fake_path = \"G:/DataSets/Fake_Dataset/CTGAN-adu100e.csv\"\n",
    "# fake_path = 'G:/DataSets/Fake_Dataset/CTABGAN-Credit150k0.csv'\n",
    "real_path = 'G:/DataSets/Credit150k.csv'\n",
    "# fake_path = \"G:/DataSets/Fake_Dataset/TransCTGAN-200eAdult3.csv\"\n",
    "#real_path = \"../CTAB-GAN-main/Real_Datasets/Adult3.csv\"\n",
    "# fake_path = \"G:/DataSets/Fake_Dataset/TVAE-200eCover.csv\"\n",
    "# real_path = 'G:\\DataSets\\Covertype30k.csv'\n",
    "# real_path = '../CTGAN-main/CTGAN-main/examples/csv/train_clean.csv'\n",
    "# fake_path = 'G:/DataSets/Fake_Dataset/Synthcity-transctgan-tit300.csv'\n",
    "#real_path = 'G:\\DataSets\\Covertype30k.csv'\n",
    "fake_path = 'G:/DataSets/Fake_Dataset/zhexian-realctgan-credit150k-25e.csv'\n",
    "# real_path = \"G:/DataSets/Churn.csv\"\n",
    "real_data = load_data(real_path)\n",
    "generated_data = load_data(fake_path)\n",
    "\n",
    "kl, jsd, wasserstein = calculate_metrics(real_data, generated_data)\n",
    "\n",
    "if kl is not None and jsd is not None and wasserstein is not None:\n",
    "    print(f\"KL Divergence: {kl}\")\n",
    "    print(f\"Jensen-Shannon Divergence: {jsd}\")\n",
    "    print(f\"Wasserstein Distance: {wasserstein}\")\n",
    "else:\n",
    "    print(\"can't be calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.decomposition import PCA\n",
    "# from scipy.stats import entropy\n",
    "# from scipy.spatial.distance import cdist\n",
    "# import scipy\n",
    "\n",
    "# def compute_jsd(real_data, generated_data, bins=30, pca_dim=10):\n",
    "#     \"\"\"\n",
    "#     Compute Jensen-Shannon Divergence (JSD) between real and generated data with PCA dimensionality reduction.\n",
    "\n",
    "#     Parameters:\n",
    "#     - real_data: Real dataset (numpy array)\n",
    "#     - generated_data: Generated dataset (numpy array)\n",
    "#     - bins: Number of bins for histogram (int)\n",
    "#     - pca_dim: Number of dimensions for PCA (int)\n",
    "#     \"\"\"\n",
    "#     assert real_data.shape == generated_data.shape, \"Real and generated data must have the same shape.\"\n",
    "    \n",
    "#     # Ensure data is in the right format\n",
    "#     real_data = np.nan_to_num(real_data)\n",
    "#     generated_data = np.nan_to_num(generated_data)\n",
    "\n",
    "#     # Apply PCA for dimensionality reduction\n",
    "#     pca = PCA(n_components=pca_dim)\n",
    "#     real_data_pca = pca.fit_transform(real_data)\n",
    "#     generated_data_pca = pca.transform(generated_data)\n",
    "\n",
    "#     # Dynamic range calculation\n",
    "#     min_vals = np.minimum(real_data_pca.min(axis=0), generated_data_pca.min(axis=0))\n",
    "#     max_vals = np.maximum(real_data_pca.max(axis=0), generated_data_pca.max(axis=0))\n",
    "#     range_vals = [(min_val, max_val) for min_val, max_val in zip(min_vals, max_vals)]\n",
    "\n",
    "#     # Compute histograms\n",
    "#     real_hist, _ = np.histogramdd(real_data_pca, bins=bins, range=range_vals, density=True)\n",
    "#     generated_hist, _ = np.histogramdd(generated_data_pca, bins=bins, range=range_vals, density=True)\n",
    "\n",
    "#     # Flatten and normalize\n",
    "#     real_hist_flat = real_hist.flatten() + 1e-8\n",
    "#     generated_hist_flat = generated_hist.flatten() + 1e-8\n",
    "#     real_hist_flat /= real_hist_flat.sum()\n",
    "#     generated_hist_flat /= generated_hist_flat.sum()\n",
    "\n",
    "#     # Compute JSD\n",
    "#     m = 0.5 * (real_hist_flat + generated_hist_flat)\n",
    "#     jsd = 0.5 * (entropy(real_hist_flat, m) + entropy(generated_hist_flat, m))\n",
    "#     return jsd\n",
    "\n",
    "# def compute_kl(real_data, generated_data, bins=30, pca_dim=10):\n",
    "#     \"\"\"\n",
    "#     Compute Kullback-Leibler Divergence (KL) between real and generated data with PCA dimensionality reduction.\n",
    "\n",
    "#     Parameters:\n",
    "#     - real_data: Real dataset (numpy array)\n",
    "#     - generated_data: Generated dataset (numpy array)\n",
    "#     - bins: Number of bins for histogram (int)\n",
    "#     - pca_dim: Number of dimensions for PCA (int)\n",
    "#     \"\"\"\n",
    "#     assert real_data.shape == generated_data.shape, \"Real and generated data must have the same shape.\"\n",
    "    \n",
    "#     # Ensure data is in the right format\n",
    "#     real_data = np.nan_to_num(real_data)\n",
    "#     generated_data = np.nan_to_num(generated_data)\n",
    "\n",
    "#     # Apply PCA for dimensionality reduction\n",
    "#     pca = PCA(n_components=pca_dim)\n",
    "#     real_data_pca = pca.fit_transform(real_data)\n",
    "#     generated_data_pca = pca.transform(generated_data)\n",
    "\n",
    "#     # Dynamic range calculation\n",
    "#     min_vals = np.minimum(real_data_pca.min(axis=0), generated_data_pca.min(axis=0))\n",
    "#     max_vals = np.maximum(real_data_pca.max(axis=0), generated_data_pca.max(axis=0))\n",
    "#     range_vals = [(min_val, max_val) for min_val, max_val in zip(min_vals, max_vals)]\n",
    "\n",
    "#     # Compute histograms\n",
    "#     real_hist, _ = np.histogramdd(real_data_pca, bins=bins, range=range_vals, density=True)\n",
    "#     generated_hist, _ = np.histogramdd(generated_data_pca, bins=bins, range=range_vals, density=True)\n",
    "\n",
    "#     # Flatten and normalize\n",
    "#     real_hist_flat = real_hist.flatten() + 1e-8\n",
    "#     generated_hist_flat = generated_hist.flatten() + 1e-8\n",
    "#     real_hist_flat /= real_hist_flat.sum()\n",
    "#     generated_hist_flat /= generated_hist_flat.sum()\n",
    "\n",
    "#     # Compute KL\n",
    "#     kl = entropy(real_hist_flat, generated_hist_flat)\n",
    "#     return kl\n",
    "\n",
    "# def compute_wd(real_data, generated_data, pca_dim=None):\n",
    "#     \"\"\"\n",
    "#     Compute Wasserstein Distance (WD) between real and generated data with optional PCA.\n",
    "\n",
    "#     Parameters:\n",
    "#     - real_data: Real dataset (numpy array)\n",
    "#     - generated_data: Generated dataset (numpy array)\n",
    "#     - pca_dim: Number of dimensions for PCA (int, optional)\n",
    "#     \"\"\"\n",
    "#     assert real_data.shape == generated_data.shape, \"Real and generated data must have the same shape.\"\n",
    "    \n",
    "#     # Ensure data is in the right format\n",
    "#     real_data = np.nan_to_num(real_data)\n",
    "#     generated_data = np.nan_to_num(generated_data)\n",
    "\n",
    "#     if pca_dim is not None and pca_dim < real_data.shape[1]:\n",
    "#         # Apply PCA for dimensionality reduction\n",
    "#         pca = PCA(n_components=pca_dim)\n",
    "#         real_data = pca.fit_transform(real_data)\n",
    "#         generated_data = pca.transform(generated_data)\n",
    "    \n",
    "#     # Compute WD\n",
    "#     wd = cdist(real_data, generated_data, metric='euclidean').mean()\n",
    "#     return wd\n",
    "\n",
    "# def evaluate_datasets(real_path, fake_path, pca_dim_js_kl=10, pca_dim_wd=None):\n",
    "#     \"\"\"\n",
    "#     Evaluate generated data against real data using WD, JSD, and KL.\n",
    "\n",
    "#     Parameters:\n",
    "#     - real_path: Path to real dataset (CSV format)\n",
    "#     - fake_path: Path to generated dataset (CSV format)\n",
    "#     - pca_dim_js_kl: Number of dimensions for PCA in JSD and KL computation (int)\n",
    "#     - pca_dim_wd: Number of dimensions for PCA in WD computation (int, optional)\n",
    "#     \"\"\"\n",
    "#     # Load datasets\n",
    "#     real_data = pd.read_csv(real_path).values\n",
    "#     generated_data = pd.read_csv(fake_path).values\n",
    "\n",
    "#     # Ensure data has the same shape\n",
    "#     assert real_data.shape[1] == generated_data.shape[1], \"Real and generated data must have the same number of features.\"\n",
    "#     min_samples = min(real_data.shape[0], generated_data.shape[0])\n",
    "#     real_data = real_data[:min_samples]\n",
    "#     generated_data = generated_data[:min_samples]\n",
    "\n",
    "#     # Compute metrics\n",
    "#     wd = compute_wd(real_data, generated_data, pca_dim_wd)\n",
    "#     jsd = compute_jsd(real_data, generated_data, pca_dim=pca_dim_js_kl)\n",
    "#     kl = compute_kl(real_data, generated_data, pca_dim=pca_dim_js_kl)\n",
    "\n",
    "#     return {\n",
    "#         \"Wasserstein Distance\": wd,\n",
    "#         \"Jensen-Shannon Divergence\": jsd,\n",
    "#         \"Kullback-Leibler Divergence\": kl\n",
    "#     }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
